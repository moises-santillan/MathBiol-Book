# 7. On the Interconnected Origins of Statistics, Probability Theory and Population Dynamics 
 

**Abstract**

This chapter traces the interconnected conceptual origins of statistics, probability theory, and population dynamics that emerged in the 17th century. It examines foundational contributions from figures such as Cardano, Pascal, Graunt, Huygens, and Bernoulli who established the theoretical and empirical bases for analyzing natural and social phenomena using mathematical and data-driven methods. Key advances discussed include the early application of quantitative methods to games of chance by Cardano, the founding of probability theory by Pascal and Fermat, Graunt's establishment of empirical demography, and the development of probabilistic population modeling through life tables. The chapter argues that this transformative period saw the beginning of quantitative, evidence-based approaches across scientific domains through linked conceptual roots in analyzing randomness, uncertainty, and human populations.

## Introduction
The fields of statistics, probability theory, and population dynamics share interconnected conceptual origins that emerged in the 17th century. During this period, mathematicians, philosophers, and early scientists began developing new empirical and mathematical approaches to understand natural and social phenomena. Where previously such domains had been studied through qualitative reasoning or abstract models, quantitative analysis utilizing data and probability concepts now took root.

Three pivotal developments laid the foundation for these disciplines as we understand them today. The first was the establishment of probability theory as a mathematical subject area through the correspondence between Blaise Pascal and Pierre de Fermat in 1654. In establishing clear definitions and analytical techniques for calculating probabilities, their work launched probability as a rigorous field of study.

Concurrently, John Graunt published his study of London mortality records titled \emph{Observations Made upon the Bills of Mortality} in 1662. This work is considered the origin of modern demography and statistics, as Graunt systematically compiled and analyzed parish death data using crude early statistical techniques. His work demonstrated the value of a quantitative, empirical approach and helped establish population modeling and life tables.

Connecting these two threads was the development of probabilistic concepts of population dynamics. Christiaan Huygens and later Nicholas Bernoulli built on Graunt's work by interpreting his partial life table probabilistically. This work in the late 17th century established the theoretical framework for life tables based on probabilistic models of mortality rates.

Together, these developments represented the interconnected foundational concepts of statistics, probability theory and population modeling that are still core to these fields today. Through increasingly mathematical and data-driven methods, scientists began developing powerful new analytical tools for understanding the natural and social world in quantitative terms.

The key sources that informed the development of this chapter were the following works :{cite}. Readers interested in exploring these topics in more depth are encouraged to refer to these publications.

## Background
The intellectual context of the 17th century set the stage for the formative developments in probability, statistics and population modeling. Advances in mathematics, along with changing philosophies of science, fostered quantitative and empirical approaches to understanding the natural world.

Mathematicians of the early 1600s had formalized algebra, introducing symbolic notation that allowed solving generalized problems rather than specific numerical examples. This mathematical maturity enabled tackling more complex real-world problems. Concurrently, Francis Bacon and others advocated inductive, experiment-based scientific methods rather than overtly theoretical deduction alone.

Early proto-statistical techniques also emerged. Blood counts recorded by Belgian Lambert van Heemskerck in 1590 showed histograms emerging as a tool to visualize data distributions. Galileo's 1611 displacement experiments laid foundations for experimental design and estimation of measurement error. 

In natural philosophy, Aristotelian teleological thought remained dominant yet faced challenges from the mechanistic views of Descartes. John Wallis' \emph{De Algebra} of 1693 signaled a shift towards mathematical representations. Investigation of celestial phenomena like comets, sunspots and nova by Kepler, Brahe and Galileo revealed complexity rather than the astronomically ``perfect" former worldview.

This context set the stage for progress. Developing numerical techniques and willingness to quantify real-world phenomena opened the doors for applying mathematics to problems like games, mortality and astronomy. Empirical natural knowledge supplanted overtly theoretical tradition, recognizing phenomena like errors and variation fundamental to scientific understanding. Challenges to established worldviews enabled new probabilistic and quantifying perspectives on inherently complex, divers phenomena. This lay groundwork for probability, statistics and life tables emerging in the mid-1600s.

## Cardano's Work
One of the earliest works to apply mathematical thinking to games of chance was Girolamo Cardano's 1526 treatise \emph{De Ludo Aleae} (\emph{On Games of Chance}). As a physician and polymath deeply engaged in mathematics, Cardano sought to determine equitable rules and stake structures for various gambling activities through probabilistic reasoning. Though he lacked a theoretical framework, Cardano's work marked a pioneering effort in what would later become probability theory.

In \emph{De Ludo Aleae}, Cardano analyzed dice, cards and various board games that involved elements of chance. For each, he aimed to calculate the mathematical expectation of each potential outcome in order to establish fair stakes. For example, in analyzing common dice games, Cardano outlined the relative probabilities of rolling different numbers and their associated payouts. However, his arithmetic calculations contained errors and he did not consistently apply proportional reasoning to determine equitable outcomes.

A problematic endeavor Cardano explored was the ``problem of points;" how winnings should be divided between two players if an incomplete game of chance was stopped before a winner was determined. While proposing solutions based on proportional reasoning, Cardano's logic contained logical gaps. For a game half-completed, he suggested stakes be split evenly rather than considering the changing probabilities as the game progressed. This flawed proportional method would continue to plague proposed solutions for some time.

Despite analytical limitations from lacking a rigorous probability system, \emph{De Ludo Aleae} marked an intellectual milestone as one of the earliest works to systematically apply mathematical and quantitative thinking to games involving chance. It helped establish chance and probability as worthwhile subjects of mathematical inquiry. While Cardano's specific solutions were imperfect, his work helped catalyze further development and paved the way for the eventual theoretical foundations later established by Pascal, Fermat, and other 17th century thinkers. 

## Pascal and Fermat
The birth of probability theory as a rigorous mathematical discipline owes to the correspondence between the French mathematician Blaise Pascal and French lawyer Pierre de Fermat in 1654. Seeking to solve the unresolved ``problem of points," their letters established fundamental concepts and analytical techniques that formed the basis of probability as a field of study. 

Pascal introduced the groundbreaking idea of mathematical expectation, which allowed calculating the average probability-weighted payoff of each possible outcome. Through examining games with varying numbers of trials like rolling dice, Pascal discovered patterns in the binomial coefficients that became known as Pascal's triangle. This arrangement reveals the coefficients of binomial expansions and allowed calculating probabilities and expectations through what is known as the binomial distribution.

Fermat independently devised an analytical approach using proportional reasoning about remaining possibilities. Through comparing solutions, Pascal and Fermat established that their methods agreed, validating that probability could be understood mathematically. They then applied these to systematically solve variations of the problem of points.

Significantly, Pascal provided clear definitions of probability, expectation, and equity in games of chance. This marked the establishment of probability theory as an organized field based on precise definitions and analytical calculations rather than intuition. Their  correspondence founded the modern mathematical understanding of probability that remains fundamental to statistics, risk analysis and related disciplines. 

## Huygens' Treatise
Building upon the foundational work of Pascal and Fermat, Christiaan Huygens' 1657 treatise \emph{De Ratiociniis in Ludo Aleae} marked an important step forward in developing probability theory. Through elegantly solving problems involving games of chance, Huygens refined key concepts and helped disseminate probabilistic reasoning to a broader intellectual audience.

Huygens presented five problems to demonstrate logical solutions using the mathematical expectation and probability techniques established earlier. His treatment expanded the scope beyond Pascal's original problem of points by considering scenarios like dividing stakes between players leaving ongoing games and determining equitable initial and ongoing stakes.

What distinguished Huygens' approach was his employment of geometric representations to arrive at solutions. By visualizing probabilities and expected values geometrically, he provided intuitive demonstrations accessible to contemporary mathematicians and scholars. Problems involving compound probability or independent events were depicted especially clearly through his innovative geometric approach.

Beyond pragmatic applications, Huygens' treatise introduced theoretical advances. He properly considered cases involving more complex stochastic processes with multiple dependent variables. The concept of independence between events was also more rigorously defined.

Huygens helped cement probability theory as a legitimate field meriting dedicated study. By refining concepts through elegant visual proof, he disseminated probabilistic reasoning to a broader intellectual community. His treatise engaged fellow mathematical thinkers while communicating foundations developed by Pascal and Fermat to a general educated readership. 

## Graunt's Bills of Mortality
While Continental mathematicians formalized probability theory, in England John Graunt published his work \emph{Observations Made upon the Bills of Mortality} in 1662. By systematically compiling and analyzing parish mortality records from London, Graunt is considered the founder of modern empirical demography and statistics.

Graunt studied weekly Bills of Mortality listing burials in each parish from 1603-1660. He assessed data quality, discussed trends in mortality from different causes, and noted seasonal patterns. By comparing burial totals to estimated births, Graunt proposed London's total population was around 460,000.

He constructed what is considered the first life table by calculating survivorship ratios at each age based on proportional mortality. While crude, this provided insights into longevity and helped estimate life expectancies. Graunt also tested hypotheses, finding for example that mortality was lower when more births occurred.

Through his careful collection and assessment of real-world evidence, Graunt established demography as an empirical discipline. Unlike earlier works grounded in theory, Graunt emphasized pragmatic, data-driven analysis and population modeling.

\emph{Observations Made upon the Bills of Mortality} represented an intellectual departure by quantitatively interrogating social phenomena. It influenced later demographers and materially aided statecraft through providing population estimates. Most significantly, Graunt established mortality records and life tables as foundations for future probabilistic population modeling developed by Huygens, Bernoulli and later thinkers. 

## Probabilistic Life Tables and Early Insurance Mathematics
Building upon Graunt's empirical establishment of life tables, later 17th century mathematicians developed probabilistic interpretations that represented important conceptual advances. Correspondence between Constantijn and Christiaan Huygens in 1669 began interpreting life tables through expected value calculations, laying early foundations.

However, the first systematic probabilistic construction and interpretation of Graunt's partial life table emerged in Nicholas Bernoulli's 1709 dissertation for the University of Basel. Seeing Graunt's observations as samples from an imagined infinite population, Bernoulli established the probability of survival and death at each age based on frequencies in the dataset.

This conceptualization described what is now known as a stationary life table based on regular and consistent mortality probabilities at each age level. Bernoulli fitted Graunt's London figures to his theoretical model to derive survival probabilities and life expectancies, demonstrating how pragmatic demographic records could inform probabilistic population modeling.

Concurrently, nascent life insurance and annuity practices drove interest in mathematically calculating fair premiums and payout values. Jan de Witt evaluated annuity pricing in 1671 based on Dutch life tables, though not fully probabilistically. Edmond Halley's 1693 paper introduced what became known as Halley's life table and outlined uses including insurance pricing, suggesting age-specific rates.

While emerging pragmatically, these developments established the analytical theoretical framework treating populations and longevity in probabilistic terms. Refined life tables coupled with probability concepts formed the mathematical underpinnings for later insurance and social policy applications informed by demographic patterns. They launched systematic population modeling central to modern actuarial science and epidemiology.

## Discussion
By the late 17th century, the conceptual foundations for statistics, probability theory, and population modeling as quantitative fields of study had begun to solidify. Individuals including Cardano, Pascal, Graunt, Huygens, Bernoulli and others made important contributions developing mathematical and empirical approaches for analyzing phenomena involving chance, data, and human populations.

Throughout this period, there was a discernible shift towards more quantitative, data-driven methods of understanding the natural and social worlds. Inspired by early empirical pioneers, scientists increasingly used numerical evidence and mathematical representations to model irregular and complex systems. They developed analytical tools to estimate parameters, assess uncertainties, and test hypotheses rigorously against observations.

Even today, statistics, probability, and population modeling remain interconnected and influence each other's ongoing progress. Advancements in one field often spur developments in related areas. For example, new statistical techniques enable analysts to more accurately model population dynamics when considering demographic uncertainties or changes over time. Similarly, probabilistic methods allow epidemiologists and other scientists to better estimate public health risks and design evidence-based policies. The significant conceptual links established in their 17th century beginnings continue shaping the nature of inquiry across these quantitative fields.